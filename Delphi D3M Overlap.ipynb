{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'atm-data-store'\n",
    "base_path = 'gp+bandit-search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(bucket, path, *args, **kwargs):\n",
    "    print('Downloading file {} from S3 bucket {}'.format(bucket, path))\n",
    "    body = bucket.Object(path).get()['Body'].read()\n",
    "    with io.BytesIO(body) as buf:\n",
    "        return pd.read_csv(buf, *args, **kwargs)\n",
    "\n",
    "def to_csv(df, bucket, key, *args, **kwargs):\n",
    "    print('Uploading file {} to S3 bucket {}'.format(bucket, key))\n",
    "    with io.StringIO() as buf:\n",
    "        df.to_csv(buf, *args, **kwargs)\n",
    "        bucket.Object(key).put(Body=buf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ATM Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def open_remote(path, compressed=False):\n",
    "    body = bucket.Object(path).get()['Body'].read()\n",
    "    \n",
    "    if compressed:\n",
    "        body = zlib.decompress(body)\n",
    "        \n",
    "    return io.BytesIO(body)\n",
    "\n",
    "with open_remote(base_path + '/csvs/datasets.csv') as f:\n",
    "    atm = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RE_ATM_SUFFIX = re.compile('_\\d*')\n",
    "\n",
    "atm['clean'] = atm.name.replace(RE_ATM_SUFFIX, '', regex=True)\n",
    "\n",
    "# Drop datasets that end up sharing the \"clean\" name.\n",
    "atm.drop_duplicates(subset=['clean'], inplace=True, keep=False)\n",
    "atm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>name</th>\n",
       "      <th>train_path</th>\n",
       "      <th>test_path</th>\n",
       "      <th>class_column</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2dplanes_1</td>\n",
       "      <td>data/processed/2dplanes_1_train.csv</td>\n",
       "      <td>data/processed/2dplanes_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>2dplanes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AP_Endometrium_Prostate_1</td>\n",
       "      <td>data/processed/AP_Endometrium_Prostate_1_train...</td>\n",
       "      <td>data/processed/AP_Endometrium_Prostate_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>APEndometriumProstate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Amazon_employee_access_1</td>\n",
       "      <td>data/processed/Amazon_employee_access_1_train.csv</td>\n",
       "      <td>data/processed/Amazon_employee_access_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazonemployeeaccess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Australian_1</td>\n",
       "      <td>data/processed/Australian_1_train.csv</td>\n",
       "      <td>data/processed/Australian_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>Australian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>BNG(breast-w)_1</td>\n",
       "      <td>data/processed/BNG(breast-w)_1_train.csv</td>\n",
       "      <td>data/processed/BNG(breast-w)_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>BNG(breast-w)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id                       name  \\\n",
       "0           1                 2dplanes_1   \n",
       "1           2  AP_Endometrium_Prostate_1   \n",
       "2           3   Amazon_employee_access_1   \n",
       "3           4               Australian_1   \n",
       "4           5            BNG(breast-w)_1   \n",
       "\n",
       "                                          train_path  \\\n",
       "0                data/processed/2dplanes_1_train.csv   \n",
       "1  data/processed/AP_Endometrium_Prostate_1_train...   \n",
       "2  data/processed/Amazon_employee_access_1_train.csv   \n",
       "3              data/processed/Australian_1_train.csv   \n",
       "4           data/processed/BNG(breast-w)_1_train.csv   \n",
       "\n",
       "                                           test_path  class_column  \\\n",
       "0                 data/processed/2dplanes_1_test.csv             0   \n",
       "1  data/processed/AP_Endometrium_Prostate_1_test.csv             0   \n",
       "2   data/processed/Amazon_employee_access_1_test.csv             0   \n",
       "3               data/processed/Australian_1_test.csv             0   \n",
       "4            data/processed/BNG(breast-w)_1_test.csv             0   \n",
       "\n",
       "                   clean  \n",
       "0               2dplanes  \n",
       "1  APEndometriumProstate  \n",
       "2   Amazonemployeeaccess  \n",
       "3             Australian  \n",
       "4          BNG(breast-w)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get D3M Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3m_bucket = s3.Bucket('d3m-data-dai')\n",
    "\n",
    "d3m_keys = [obj.key for obj in d3m_bucket.objects.filter(Prefix='datasets')]\n",
    "\n",
    "RE_DATASET = re.compile('^datasets/(.*)\\.tar\\.gz')\n",
    "\n",
    "d3m_datasets = [RE_DATASET.match(key).group(1) for key in d3m_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_D3M_PREFIX = re.compile('^[^_]+_(\\d*_)?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3m = pd.DataFrame({'name': d3m_datasets})\n",
    "d3m['clean'] = d3m.name.replace(RE_D3M_PREFIX, '', regex=True)\n",
    "\n",
    "# Drop datasets that end up sharing the \"clean\" name.\n",
    "d3m.drop_duplicates(subset=['clean'], keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>124_120_mnist</td>\n",
       "      <td>mnist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124_138_cifar100</td>\n",
       "      <td>cifar100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124_153_svhn_cropped</td>\n",
       "      <td>svhn_cropped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124_174_cifar10</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124_178_coil100</td>\n",
       "      <td>coil100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name         clean\n",
       "0         124_120_mnist         mnist\n",
       "1      124_138_cifar100      cifar100\n",
       "2  124_153_svhn_cropped  svhn_cropped\n",
       "3       124_174_cifar10       cifar10\n",
       "4       124_178_coil100       coil100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = atm[atm.clean.isin(d3m.clean)].copy()\n",
    "merged = match.merge(d3m, on='clean', suffixes=('', '_d3m'))\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>name</th>\n",
       "      <th>train_path</th>\n",
       "      <th>test_path</th>\n",
       "      <th>class_column</th>\n",
       "      <th>clean</th>\n",
       "      <th>name_d3m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Australian_1</td>\n",
       "      <td>data/processed/Australian_1_train.csv</td>\n",
       "      <td>data/processed/Australian_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>Australian</td>\n",
       "      <td>LL0_40509_Australian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>CostaMadre1_1</td>\n",
       "      <td>data/processed/CostaMadre1_1_train.csv</td>\n",
       "      <td>data/processed/CostaMadre1_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>CostaMadre1</td>\n",
       "      <td>LL0_1446_CostaMadre1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>MegaWatt1_1</td>\n",
       "      <td>data/processed/MegaWatt1_1_train.csv</td>\n",
       "      <td>data/processed/MegaWatt1_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>MegaWatt1</td>\n",
       "      <td>LL0_1442_MegaWatt1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>SPECT_1</td>\n",
       "      <td>data/processed/SPECT_1_train.csv</td>\n",
       "      <td>data/processed/SPECT_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>SPECT</td>\n",
       "      <td>uu4_SPECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>ailerons_1</td>\n",
       "      <td>data/processed/ailerons_1_train.csv</td>\n",
       "      <td>data/processed/ailerons_1_test.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>ailerons</td>\n",
       "      <td>LL0_296_ailerons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id           name                              train_path  \\\n",
       "0           4   Australian_1   data/processed/Australian_1_train.csv   \n",
       "1          12  CostaMadre1_1  data/processed/CostaMadre1_1_train.csv   \n",
       "2          16    MegaWatt1_1    data/processed/MegaWatt1_1_train.csv   \n",
       "3          26        SPECT_1        data/processed/SPECT_1_train.csv   \n",
       "4          33     ailerons_1     data/processed/ailerons_1_train.csv   \n",
       "\n",
       "                               test_path  class_column        clean  \\\n",
       "0   data/processed/Australian_1_test.csv             0   Australian   \n",
       "1  data/processed/CostaMadre1_1_test.csv             0  CostaMadre1   \n",
       "2    data/processed/MegaWatt1_1_test.csv             0    MegaWatt1   \n",
       "3        data/processed/SPECT_1_test.csv             0        SPECT   \n",
       "4     data/processed/ailerons_1_test.csv             0     ailerons   \n",
       "\n",
       "               name_d3m  \n",
       "0  LL0_40509_Australian  \n",
       "1  LL0_1446_CostaMadre1  \n",
       "2    LL0_1442_MegaWatt1  \n",
       "3             uu4_SPECT  \n",
       "4      LL0_296_ailerons  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('csvs/d3m_atm_overlap.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
